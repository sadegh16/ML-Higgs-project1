# -*- coding: utf-8 -*-
"""ML-epfl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jZjvJMFcI9DPsNbRdEgIdu4DWbBbRkTN
"""

import csv
import numpy as np
import matplotlib.pyplot as plt


# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/ML-epfl/data

"""some helper functions for project 1."""

val_X=None
val_y=None
def load_csv_data(data_path, sub_sample=False):
    """Loads data and returns y (class labels), tX (features) and ids (event ids)"""
    y = np.genfromtxt(data_path, delimiter=",", skip_header=1, dtype=str, usecols=1)
    x = np.genfromtxt(data_path, delimiter=",", skip_header=1)
    ids = x[:, 0].astype(np.int)
    input_data = x[:, 2:]

    # convert class labels from strings to binary (-1,1)
    yb = np.ones(len(y))
    yb[np.where(y=='b')] = -1

    # sub-sample
    if sub_sample:
        yb = yb[::50]
        input_data = input_data[::50]
        ids = ids[::50]

    return yb, input_data, ids

def predict_labels(weights, data):
    """Generates class predictions given weights, and a test data matrix"""
    y_pred = np.dot(data, weights)
    y_pred[np.where(y_pred <= 0.5)] = -1
    y_pred[np.where(y_pred > 0.5)] = 1

    return y_pred

def create_csv_submission(ids, y_pred, name):
    """
    Creates an output file in .csv format for submission to Kaggle or AIcrowd
    Arguments: ids (event ids associated with each prediction)
               y_pred (predicted class labels)
               name (string name of .csv output file to be created)
    """
    with open(name, 'w') as csvfile:
        fieldnames = ['Id', 'Prediction']
        writer = csv.DictWriter(csvfile, delimiter=",", fieldnames=fieldnames)
        writer.writeheader()
        for r1, r2 in zip(ids, y_pred):
            writer.writerow({'Id':int(r1),'Prediction':int(r2)})


def find_acc(y,y_hat):
  """
  give y and y_hat based on the threshold of 0.5, we compute
  the accuracy

  """
  y,y_hat=np.copy(y), np.copy(y_hat)
  y[np.where(y <= 0.5)] = -1
  y[np.where(y > 0.5)] = 1
  y_hat[np.where(y_hat <= 0.5)] = -1
  y_hat[np.where(y_hat > 0.5)] = 1
  return np.sum(y==y_hat)/len(y)



def poly_M(X,m):

  """
  This function creates new X ( feature matrix ) by concatinating
  X^i from the primitive X matrix for i from 2 to m 

  """

  # find nan data positions
  nan_ids=X==-999
  nan_ids_tmp=np.copy(nan_ids)
  X_tmp=np.copy(X)

  # this loop is used to add power i of X to X on each iteration and finally hold the positions of nans with -999
  for i in range(2,m+1): 
    X = np.c_[X_tmp**i, X]
    nan_ids= np.c_[nan_ids_tmp, nan_ids]
  X[nan_ids]=-999
  return X

def featureNormalize(X):

  """
  Normalize X by minimizing its mean and deviding by standard deviation
  Note:  we replace -999 elements with 0 after normalaizing by other element of each column
  """
  X_norm = np.copy(X)
  mu = np.zeros(X.shape[1])
  sigma = np.zeros(X.shape[1])

  # finding mean and var without considering -999s
  for j in range(X.shape[1]): 
    nonnan_ids = np.where(X[:,j]!=-999)[0]
    mu[j]=X[nonnan_ids,j].mean()
    sigma[j] = X[nonnan_ids,j].std()
  
  # replacing -999 values with zero ( mean )
  for j in range(X.shape[1]): 
    nan_ids = np.where(X[:,j]==-999)[0]
    X_norm[:,j] = (X[:,j]-mu[j])/sigma[j]
    X_norm[nan_ids,j] = 0

  return X_norm, mu, sigma


def prepare_input_data(data,degree=1):
  """
  prepareing the input data
  """
  # valid_rows=np.any(input_data==-999,axis=1)
  # yb, input_data, ids=yb[valid_rows], input_data[valid_rows], ids[valid_rows]

  valid_columns=None
  # valid_columns=np.sum(data==-999,axis=0)<0.3*len(data)
  # data=data[:,valid_columns]
  data= poly_M(data,degree)
  data, mu, sigma = featureNormalize(data)
  m,n= data.shape

  # add a column with number 1 as the bias
  data = np.c_[np.ones((m,1)), data]
  return data, valid_columns,mu, sigma 




def prepare_test_data(data,mu, sigma,degree=1):

  data= poly_M(data,degree)
    
  for j in range(data.shape[1]): 
    nan_ids = np.where(data[:,j]==-999)[0]
    data[:,j] = (data[:,j]-mu[j])/sigma[j]
    data[nan_ids,j] = 0

  m,n= data.shape
  data = np.c_[np.ones((m,1)), data]
  return data

# NOT USED
def upsample_minority(y, X):

  """
  this function is used to upsample minority class 
  """

  minority_class_ids = np.where(y==1)[0]
  majority_class_ids = np.where(y==0)[0]
  extra_minority_ids= np.random.choice(minority_class_ids,
                                       len(majority_class_ids)-len(minority_class_ids),
                                       replace=True)
  new_minority_class_ids= np.concatenate([minority_class_ids , extra_minority_ids])


  return  np.concatenate([y , y[extra_minority_ids]]), np.concatenate([X , X[extra_minority_ids]])




def train_val_split(y, X):

  """
  devide input data to train and validation (80% and 20%)

  """
  train_size= int(X.shape[0]*0.8)
  val_size= X.shape[0]- train_size
  # adding a seed to make randomization determinestic
  indices = np.random.RandomState(seed=42).permutation(X.shape[0])
  training_idx, val_idx = indices[:train_size], indices[val_size:]
  return X[training_idx,:], X[val_idx,:], y[training_idx], y[val_idx]

def set_val_data(val_X_t,val_y_t):
    global val_X, val_y
    val_X, val_y=val_X_t,val_y_t


def computeCost(X, y, theta):
  m = len(y);
  h = np.matmul(X,theta);
  return 1/(2*m)*np.linalg.norm(h-y,ord=2)**2;

def least_squares(y, X):
  X_T=np.transpose(X)
  theta = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_T,X)),X_T),y)
  train_loss=computeCost(X, y, theta)
  val_loss=computeCost(val_X, val_y, theta)
  train_acc = find_acc(y, np.matmul(X,theta))
  val_acc = find_acc(val_y, np.matmul(val_X,theta))
  print('Training acc= {:.6f} Val acc= {:.6f}'.format(train_acc, val_acc))
  return theta
# weight=least_squares(train_y, train_X);

def least_squares_GD(y, X, theta, num_iters, gamma, show=False):
  m = len(y);
  train_loss= np.zeros(num_iters);
  val_loss = np.zeros(num_iters);
  train_acc= np.zeros(num_iters);
  val_acc = np.zeros(num_iters);
  models=[]
  for iter in range (num_iters):
    
    # update weights
    theta = theta - gamma/m * np.matmul(np.transpose(X),(np.matmul(X,theta)-y))
    # compute losses
    train_loss[iter] = computeCost(X, y, theta)
    val_loss[iter] = computeCost(val_X, val_y, theta)
    train_acc[iter] = find_acc(y, np.matmul(X,theta))
    val_acc[iter] = find_acc(val_y, np.matmul(val_X,theta))
    models.append(np.copy(theta))
    print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[iter], val_loss[iter], train_acc[iter], val_acc[iter]))
  # finding the best model using val set
  best_model_idx=np.argmin(val_loss)
  print("Best model info *** :")
  print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[best_model_idx],val_loss[best_model_idx],
                                                                                                         train_acc[best_model_idx],val_acc[best_model_idx],))
  
  if show:
    plt.plot(range(1,num_iters+1), train_acc)
    plt.plot(range(1,num_iters+1), train_acc)
    plt.xlabel('epoch')
    plt.ylabel('accuracy')
    plt.legend(['training', 'validation'], loc='upper right')
    plt.show()
  return models[best_model_idx], train_loss[best_model_idx]


# least_squares_GD(train_y,train_X,np.random.rand(train_X.shape[1]),5000,0.01,show=True)

def least_squares_SGD(y, X, theta, num_iters, gamma, show=False):
    m = len(y);
    train_loss= np.zeros(num_iters);
    val_loss = np.zeros(num_iters);
    train_acc= np.zeros(num_iters);
    val_acc = np.zeros(num_iters);
    models=[]
    for iter in range (num_iters):
      random_idx=np.random.randint(m)
      # update weights
      theta = theta - gamma * np.transpose(X[random_idx])*(np.matmul(X[random_idx],theta)-y[random_idx])
      train_loss[iter] = computeCost(X, y, theta)
      val_loss[iter] = computeCost(val_X, val_y, theta)
      train_acc[iter] = find_acc(y, np.matmul(X,theta))
      val_acc[iter] = find_acc(val_y, np.matmul(val_X,theta))
      models.append(np.copy(theta))
      print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[iter], val_loss[iter],
                                                                                                              train_acc[iter], val_acc[iter]))
    # finding the best model using val set
    best_model_idx=np.argmin(val_loss)
    print("Best model info *** :")
    print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[best_model_idx],val_loss[best_model_idx],
                                                                                                         train_acc[best_model_idx],val_acc[best_model_idx],))
  
    if show:
      plt.plot(range(1,num_iters+1), train_loss)
      plt.plot(range(1,num_iters+1), val_loss)
      plt.xlabel('epoch')
      plt.ylabel('loss')
      plt.legend(['training', 'validation'], loc='upper right')
      plt.show()
    return models[best_model_idx], train_loss[best_model_idx]
# least_squares_SGD(train_y,train_X,np.random.rand(train_X.shape[1]),10000,0.001,show=True)


def computeCost_ridge(X, y, theta, lambda_):
  m = len(y);
  h = np.matmul(X,theta);
  return 1/(2*m)*np.linalg.norm(h-y,ord=2)**2+ lambda_*np.linalg.norm(theta,ord=2)**2

def ridge_regression(y, X, lambda_):
  X_T=np.transpose(X)
  n=len(X_T)
  m = len(y)
  I=np.identity(n)
  lambda_prime=lambda_*(2*m);
  # find the final weights
  theta = np.matmul(np.matmul(np.linalg.inv(np.matmul(X_T,X)+lambda_prime*I),X_T),y)
  train_loss=computeCost_ridge(X, y, theta, lambda_)
  val_loss=computeCost_ridge(val_X, val_y, theta, lambda_)
  train_acc = find_acc(y, np.matmul(X,theta))
  val_acc = find_acc(val_y, np.matmul(val_X,theta))
  print('Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(train_loss, 
                                                                                               val_loss,
                                                                                               train_acc, val_acc))
  return theta
# ridge_regression(train_y, train_X, 0);

def sigmoid(z):
  return 1/(1+np.exp(-z))
def logistic_regression(y, X, theta, num_iters, gamma, show=False):
  # change dtype not to overflow
  y=y.astype(dtype=np.float128)
  X=X.astype(dtype=np.float128)
  theta=theta.astype(dtype=np.float128)

  m = len(y);
  train_loss= np.zeros(num_iters)
  val_loss = np.zeros(num_iters)
  train_acc= np.zeros(num_iters)
  val_acc = np.zeros(num_iters)
  models=[]

  for iter in range (num_iters):
    h = sigmoid(np.matmul(X,theta))
    train_loss[iter] =  1/m*np.sum(np.log(1+np.exp(np.matmul(X,theta)))-y*np.matmul(X,theta))
    # update weights
    theta = theta - gamma * 1/m *np.matmul(np.transpose(X),(h-y))
    val_h = sigmoid(np.matmul(val_X,theta))
    val_loss[iter] =  1/m*np.sum(np.log(1+np.exp(np.matmul(val_X,theta)))-val_y*np.matmul(val_X,theta))
    train_acc[iter] = find_acc(y, h)
    val_acc[iter] = find_acc(val_y, val_h)
    models.append(np.copy(theta))
    print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[iter], val_loss[iter], train_acc[iter], val_acc[iter]))

  # finding the best model using val set
  best_model_idx=np.argmin(val_loss)
  print("Best model info *** :")
  print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[best_model_idx],val_loss[best_model_idx],
                                                                                                         train_acc[best_model_idx],val_acc[best_model_idx],))
  if show:
    plt.plot(range(1,num_iters+1), train_acc)
    plt.plot(range(1,num_iters+1), val_acc)
    plt.xlabel('epoch')
    plt.ylabel('acc')
    plt.legend(['training', 'validation'], loc='upper right')
    plt.show()
  return models[best_model_idx], train_loss[best_model_idx]
# logistic_regression(train_y,train_X,np.random.rand(train_X.shape[1]),1000,0.1,show=True)

def sigmoid(z):
  return 1/(1+np.exp(-z))
def reg_logistic_regression(y, X, lambda_, theta, num_iters, gamma, show=False):

  # change dtype not to overflow
  y=y.astype(dtype=np.float128)
  X=X.astype(dtype=np.float128)
  theta=theta.astype(dtype=np.float128)


  m = len(y);
  train_loss= np.zeros(num_iters)
  val_loss = np.zeros(num_iters)
  train_acc= np.zeros(num_iters)
  val_acc = np.zeros(num_iters)
  models=[]

  for iter in range (num_iters):
    h = sigmoid(np.matmul(X,theta))
    train_loss[iter] =  1/m*np.sum(np.log(1+np.exp(np.matmul(X,theta)))-y*np.matmul(X,theta))+ lambda_/(2*m)*np.linalg.norm(theta[1:],ord=2)**2
    # update weights
    theta = theta - gamma * 1/m * ( np.matmul(np.transpose(X),(h-y)) + lambda_/m *np.array( [0]+ list(theta)[1:]))
    val_h = sigmoid(np.matmul(val_X,theta))
    val_loss[iter] =  1/m*np.sum(np.log(1+np.exp(np.matmul(val_X,theta)))-val_y*np.matmul(val_X,theta)) + lambda_/(2*len(val_y))*np.linalg.norm(theta[1:],ord=2)**2
    train_acc[iter] = find_acc(y, h)
    val_acc[iter] = find_acc(val_y, val_h)
    models.append(np.copy(theta))
    print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[iter], val_loss[iter],
                                                                                                           train_acc[iter], val_acc[iter]))

  # finding the best model using val set
  best_model_idx=np.argmin(val_loss)
  print("Best model info *** :")
  print('Epoch {:d}: Training loss= {:.6f} Val loss= {:.6f} Training acc= {:.6f} Val acc= {:.6f}'.format(iter, train_loss[best_model_idx],val_loss[best_model_idx],
                                                                                                         train_acc[best_model_idx],val_acc[best_model_idx],))
  
  if show:
    plt.plot(range(1,num_iters+1), train_loss)
    plt.plot(range(1,num_iters+1), val_loss)
    plt.xlabel('epoch')
    plt.ylabel('acc')
    plt.legend(['training', 'validation'], loc='upper right')
    plt.show()
  return models[best_model_idx], train_loss[best_model_idx]
# reg_logistic_regression(train_y,train_X,0.5, np.random.rand(train_X.shape[1]),1000,0.1,show=True)

####### This bunch of code is used to plot ########
    # plt.plot(range(1,1000+1), train_loss1)
    # plt.plot(range(1,1000+1), val_loss1)
    # plt.plot(range(1,2000+1), train_loss2)
    # plt.plot(range(1,2000+1), val_loss2)
    # plt.xlabel('epoch')
    # plt.ylabel('accuracy')
    # plt.legend(['train lambda= 0.1','validation lambda= 0.1',], loc='upper right')
    # # plt.show()
    # plt.savefig("acc_reg_logisticGD_train_val")

##### find the predictions #######

